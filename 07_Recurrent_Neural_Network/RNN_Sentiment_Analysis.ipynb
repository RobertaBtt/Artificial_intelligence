{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Sentiment Analysis"
      ],
      "metadata": {
        "id": "GBdHkhjUjJn-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "This is a second prediction exercise with RNN but using in this case data in text format. To do this, we will use the https://www.kaggle.com/datasets/ankurzing/sentiment-analysis-for-financial-news  dataset, where financial news headlines appear, and along with it a category indicating whether that text corresponds to a positive, negative or neutral sentiment. The categories of feelings are:\n",
        "▪ Positive.\n",
        "▪ Negative\n",
        "▪ Neutral.\n"
      ],
      "metadata": {
        "id": "l2Vd7CAwjFpV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "import gensim.downloader as api\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.layers import Dense, LSTM, Dropout\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import f1_score, confusion_matrix, precision_score,recall_score, log_loss\n",
        "from imblearn.under_sampling import RandomUnderSampler\n",
        "from sklearn.preprocessing import LabelEncoder"
      ],
      "metadata": {
        "id": "pfomgtB5jvgK"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#============================================================================\n",
        "# 1. Load Data\n",
        "#============================================================================\n",
        "# Load files\n",
        "tf.random.set_seed(42)\n",
        "path_files = \"datasets/bbc-fulltext-and-category\"\n",
        "df_raw = pd.read_csv(path_files+'/bbc-text.csv')\n",
        "# Shuffle input\n",
        "df_raw = df_raw.sample(frac=1)"
      ],
      "metadata": {
        "id": "Ez3CIi5kmkHP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load word2vec\n",
        "word_vectors = api.load(\"glove-wiki-gigaword-100\")\n",
        "vocabulary = [x for x in word_vectors.vocab]\n",
        "# Set lemmatizer\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "# Check embeddings of one word\n",
        "vector = word_vectors['computer']\n",
        "print(vector)\n",
        "# Label encoding\n",
        "lb = LabelEncoder()\n",
        "df_raw['category'] = lb.fit_transform(df_raw['category'])\n",
        "X = pd.DataFrame(df_raw['text'])\n",
        "y = df_raw['category']"
      ],
      "metadata": {
        "id": "B9EZzPbyoKFM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#============================================================================\n",
        "# 0. General functions\n",
        "#============================================================================\n",
        "def word_vector(df_input, lemmatizer, word_vectors, vocabulary, col_sentences):\n",
        " \"\"\"\n",
        " Function to preprocess the input words and get a list with\n",
        " the embeddings arrays of the words in each record.\n",
        " Parameters\n",
        " ----------\n",
        " df_input : dataframe\n",
        " input dataframe with all texts.\n",
        " lemmatizer : object\n",
        " NLTK stemming object.\n",
        " word_vectors : object\n",
        " object with the word2vecs of the Gensim vocabulary.\n",
        " vocabulary : list\n",
        " List of existing words in Gensim's vocabulary.\n",
        " col_sentences : str\n",
        " column of the dataframe where the phrases are.\n",
        " Returns\n",
        " -------\n",
        " X : list\n",
        " List of lists in which each record has the list with the arrays\n",
        " of the embeddings of the words of that phrase. That is, X[0] has\n",
        " a list where each element corresponds to the embeddings of a word.\n",
        " Thus, for example, X[0][2] will be a vector of dimension 100 where it\n",
        "appears\n",
        " the embeddings vector of the third word of the first sentence.\n",
        " \"\"\"\n",
        "\n",
        "\n",
        " X = []\n",
        "\n",
        " for text in df_input[col_sentences]:\n",
        "\n",
        " # Tokenize every phrase\n",
        " # Change all to lower case\n",
        "  words = re.findall(r'\\w+', text.lower(),flags = re.UNICODE)\n",
        "\n",
        "  # Elimination of stop_words\n",
        "  words = [word for word in words if word not in\n",
        "  stopwords.words('english')]\n",
        "  # Remove hyphens and other weird symbols\n",
        "  words = [word for word in words if not word.isdigit()] # Elimino numeros\n",
        "  # Stemming\n",
        "  words = [lemmatizer.lemmatize(w) for w in words]\n",
        "  # Delete words that are not in the vocabulary\n",
        "  words = [word for word in words if word in vocabulary]\n",
        "  # Word2Vec\n",
        "  words_embeddings = [word_vectors[x] for x in words]\n",
        "\n",
        "  # Save the final sentence\n",
        "  X.append(words_embeddings) # save as a numpy array\n",
        "\n",
        " return X\n"
      ],
      "metadata": {
        "id": "p85yTHgfpJGP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#===========================================================================\n",
        "# 2. Preprocess\n",
        "#===========================================================================\n",
        "# Obtain X variable and prepare y.\n",
        "X = word_vector(X,\n",
        " lemmatizer,\n",
        " word_vectors,\n",
        " vocabulary,\n",
        " col_sentences=\"text\")"
      ],
      "metadata": {
        "id": "FR2Y_xnMpepP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# One-hot encode output\n",
        "y = to_categorical(y)\n",
        "\n",
        "# Train/Test split\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "X, y, test_size=0.25, random_state=42)\n",
        "\n",
        "# Obtain tensor: [N_SENTENCES x SEQ_LENGTH x EMBEDDING_FEATURES]\n",
        "SEQ_LENGTH = np.int(np.round(np.percentile([len(x) for x in X], 99,interpolation = 'midpoint')))\n",
        "\n",
        "# SEQ_LENGTH = np.int(np.round(np.percentile([len(x) for x in X], 100,interpolation = 'midpoint')))\n",
        "\n",
        "data_train = pad_sequences(X_train, maxlen=SEQ_LENGTH, padding=\"post\", truncating=\"post\")\n",
        "data_test = pad_sequences(X_test,maxlen=SEQ_LENGTH,padding=\"post\",truncating=\"post\")"
      ],
      "metadata": {
        "id": "16MA6i-kqMZE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def create_RNN(x_train, K, n_lstm=8, loss='categorical_crossentropy',\n",
        "optimizer='adam'):\n",
        " \"\"\"\n",
        " Function to create the RNN. As input parameter we only need the array\n",
        " of features to specify the input dimensionality of the NN.\n",
        " Parameters\n",
        " ----------\n",
        " x_input : array\n",
        " Input feature matrix.\n",
        " K: int\n",
        " Clases de salida\n",
        " n_lstm : int, optional\n",
        " Number of lstm used. The default is 8.\n",
        " loss : string, optional\n",
        " loss metric. The default is 'categorical_crossentropy'.\n",
        " optimizer : string, optional\n",
        " optimizer. The default is 'adam'.\n",
        " Returns\n",
        " -------\n",
        " model : object\n",
        "Advanced deep learning – Supervised deep learning (II)\n",
        "83 © Structuralia\n",
        " Trained model.\n",
        " \"\"\"\n",
        "\n",
        " # Begin sequence\n",
        " model = tf.keras.Sequential()\n",
        "\n",
        " # Add a LSTM layer with 8 internal units.\n",
        " model.add(LSTM(n_lstm, input_shape=x_train.shape[-2:]))\n",
        "\n",
        " # Add Dropout\n",
        " # model.add(Dropout(0.5))\n",
        "\n",
        " # # Another layer\n",
        " # model.add(Dense(100, activation='relu'))\n",
        "\n",
        " # # Output\n",
        " model.add(Dense(K, activation='softmax'))\n",
        "\n",
        " # Compile model\n",
        " model.compile(loss=loss, optimizer=optimizer)\n",
        "\n",
        " return model\n",
        "# ===========================================================================\n",
        "# 3. Train model\n",
        "# ===========================================================================\n",
        "# Params\n",
        "# M = 50 # hidden layer size\n",
        "K = y_train.shape[1] # N classes\n",
        "# V = data_train.shape[2] # EMBEDDING_FEATURES\n",
        "batch_size = 500\n",
        "epochs = 100\n",
        "# Create RNN\n",
        "model = create_RNN(x_train = data_train,K = K, n_lstm = 200,\n",
        "loss = 'categorical_crossentropy', optimizer = 'adam')\n",
        "print(model.summary())\n",
        "# Fit model\n",
        "model.fit(data_train,y_train,epochs = epochs,batch_size = batch_size)\n",
        "# Save model\n",
        "model.save('model_nlp_reviews2.h5')"
      ],
      "metadata": {
        "id": "iR15GS7nqiqd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Finally, we evaluate the model"
      ],
      "metadata": {
        "id": "LkLVVpSpq4zU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ===========================================================================\n",
        "# 4. Evaluate\n",
        "# ===========================================================================\n",
        "# Obtain predictions\n",
        "y_pred = model.predict(data_test)\n",
        "\n",
        "# Obtain original values (not one-hot encoded)\n",
        "if type(y_test) != list:\n",
        "  y_test = [np.argmax(x) for x in y_test]\n",
        "\n",
        "y_pred = [np.argmax(x) for x in y_pred]\n",
        "# Evaluate results\n",
        "cm = confusion_matrix(y_test, y_pred)\n",
        "print(\"Confusion Matrix: \", cm)\n",
        "print(\"Precision: \", precision_score(y_test, y_pred, average='macro'))\n",
        "print(\"Recall: \", recall_score(y_test, y_pred, average='macro'))\n",
        "print(\"f1_score: \", f1_score(y_test, y_pred, average='macro'))\n"
      ],
      "metadata": {
        "id": "SsN-CA94q6dw"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}