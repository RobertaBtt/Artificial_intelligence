{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "# Neural Network optimization"
      ],
      "metadata": {
        "id": "7ji12tt4ewUn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Each time a neural network finishes passing a batch through the network and generates prediction\n",
        "results, it must decide how to use the difference between the results obtained and the values that\n",
        "it knows to be true to adjust the weights at the nodes so that the network moves towards a\n",
        "solution. The algorithm that determines this step is known as the optimization algorithm."
      ],
      "metadata": {
        "id": "4oh1RDXnrC7f"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2.1 Practical example"
      ],
      "metadata": {
        "id": "giKkPkvwrEkQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import keras\n",
        "from keras.datasets import mnist\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Dropout\n",
        "from keras.optimizers import RMSprop\n",
        "import time\n",
        "import matplotlib.pyplot as plt"
      ],
      "metadata": {
        "id": "aXbMM7pErL5H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "batch_size = 128\n",
        "num_classes = 10\n",
        "epochs = 20\n",
        "\n",
        "#train test split data\n",
        "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
        "\n",
        "x_train = x_train.reshape(60000,784)\n",
        "x_test = x_test.reshape(10000, 784)\n",
        "x_train = x_train.astype('float32')\n",
        "x_test = x_test.astype('float32')\n",
        "x_train /= 255\n",
        "x_test /= 255\n",
        "print(x_train.shape[0], 'train samples')\n",
        "print(x_test.shape[0], 'test samples')\n",
        "\n",
        "#convert class vectors to binary class matrices\n",
        "y_train = keras.utils.to_categorical(y_train, num_classes)\n",
        "y_test = keras.utils.to_categorical(y_test, num_classes)\n",
        "\n",
        "model = Sequential()\n",
        "model.add(Dense(512, activation='relu', input_shape=(784,)))\n",
        "model.add(Dropout(0.2))\n",
        "model.add(Dense(512, activation='relu'))\n",
        "model.add(Dropout(0.2))\n",
        "model.add(Dense(num_classes, activation='softmax'))\n",
        "\n",
        "model.summary()"
      ],
      "metadata": {
        "id": "T6-Eov3urRuV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### SGD\n",
        "SGD, or stochastic gradient descent, is the “classic” optimization algorithm.\n",
        "\n",
        "In SGD the gradient\n",
        "of the network loss function with respect to each individual weight in the network is computed.\n",
        "\n",
        "---\n",
        "\n",
        "Each direct pass through the network results in a certain parameterized loss function, and we use\n",
        "each of the gradients that we have created for each of the weights, multiplied by a certain learning\n",
        "rate, to move our weights in the direction that their gradient points.\n",
        "\n",
        "---\n",
        "\n",
        "SGD is the simplest algorithm both conceptually and in terms of its behavior. Given a small\n",
        "enough learning rate, SGD always follows the gradient on the cost surface.\n",
        "\n",
        "The new weights\n",
        "generated in each iteration will always be strictly better than the previous ones from the previous\n",
        "iteration.\n",
        "\n",
        "---\n",
        "\n",
        "SGD's simplicity makes it a good choice for shallow networks. However, it also means that SGD\n",
        "converges significantly slower than other more advanced algorithms that are also available in\n",
        "keras. It is also less able to escape local minima on the cost surface."
      ],
      "metadata": {
        "id": "dCLM7VLsrcF8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "\n",
        "\n",
        "'''\n",
        "\n",
        "#=========================== SGD =============================================\n",
        "model.compile(loss = 'categorical_crossentropy',\n",
        "              optimizer = 'SGD',\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "timeStart = time.time()\n",
        "\n",
        "history = model.fit(x_train, y_train,\n",
        "                    batch_size = batch_size,\n",
        "                    epochs = epochs,\n",
        "                    verbose=1,\n",
        "                    validation_data=(x_test, y_test))\n",
        "\n",
        "timeFinal = time.time() - timeStart\n",
        "\n",
        "score = model.evaluate(x_test, y_test, verbose=0)\n",
        "print('Test loss:', score[0])\n",
        "print('Test accuracy:', score[1])\n",
        "print('Time:', timeFinal)\n",
        "\n",
        "\n",
        "plt.plot(history.history['accuracy'])\n",
        "plt.plot(history.history['val_accuracy'])\n",
        "plt.title('Model Accuracy')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.xlabel('Epoch')\n",
        "plt.legend(['Train','Test'], loc='upper left')\n",
        "plt.show()\n",
        "\n",
        "'''\n",
        "'''\n",
        "#=========================== RMSprop =============================================\n",
        "model.compile(loss = 'categorical_crossentropy',\n",
        "              optimizer = 'RMSprop',\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "timeStart = time.time()\n",
        "\n",
        "history = model.fit(x_train, y_train,\n",
        "                    batch_size = batch_size,\n",
        "                    epochs = epochs,\n",
        "                    verbose=1,\n",
        "                    validation_data=(x_test, y_test))\n",
        "\n",
        "timeFinal = time.time() - timeStart\n",
        "\n",
        "score = model.evaluate(x_test, y_test, verbose=0)\n",
        "print('Test loss:', score[0])\n",
        "print('Test accuracy:', score[1])\n",
        "print('Time:', timeFinal)\n",
        "\n",
        "\n",
        "plt.plot(history.history['accuracy'])\n",
        "plt.plot(history.history['val_accuracy'])\n",
        "plt.title('Model Accuracy')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.xlabel('Epoch')\n",
        "plt.legend(['Train','Test'], loc='upper left')\n",
        "plt.show()\n",
        "\n",
        "'''\n",
        "\n",
        "\n",
        "#=========================== ADAM =============================================\n",
        "model.compile(loss = 'categorical_crossentropy',\n",
        "              optimizer = 'adam',\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "timeStart = time.time()\n",
        "\n",
        "history = model.fit(x_train, y_train,\n",
        "                    batch_size = batch_size,\n",
        "                    epochs = epochs,\n",
        "                    verbose=1,\n",
        "                    validation_data=(x_test, y_test))\n",
        "\n",
        "timeFinal = time.time() - timeStart\n",
        "\n",
        "score = model.evaluate(x_test, y_test, verbose=0)\n",
        "print('Test loss:', score[0])\n",
        "print('Test accuracy:', score[1])\n",
        "print('Time:', timeFinal)\n",
        "\n",
        "\n",
        "plt.plot(history.history['accuracy'])\n",
        "plt.plot(history.history['val_accuracy'])\n",
        "plt.title('Model Accuracy')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.xlabel('Epoch')\n",
        "plt.legend(['Train','Test'], loc='upper left')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "4q9XnCrCrHtd"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}