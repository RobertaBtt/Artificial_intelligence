{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "# Multi Layer Perceptron (MLP)"
      ],
      "metadata": {
        "id": "7ji12tt4ewUn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The quintessential example of a deep learning model is the deep feedforward network, multilayer\n",
        "perceptron (MLP). A multilayer perceptron is simply a mathematical function that maps a set of\n",
        "input values to output values."
      ],
      "metadata": {
        "id": "erpu5Gh_IaAS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The function is formed by putting many simpler functions together. We can think of each\n",
        "application of a different mathematical function as a new representation of the input."
      ],
      "metadata": {
        "id": "I4a4G-Pc7byM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "First, we are going to develop our own library to carry out the definition, training and prediction of\n",
        "our own MLP deep neural network."
      ],
      "metadata": {
        "id": "oGDM4xYF7kwC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n"
      ],
      "metadata": {
        "id": "qq-KLvR27sbu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Define the activation functions to be used\n",
        "\n",
        "* sigmoid\n",
        "* sigmoid_derivative\n",
        "* relu"
      ],
      "metadata": {
        "id": "Vq7g6lhJ7trW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "class Mlp():\n",
        "    '''\n",
        "    fully-connected Multi-Layer Perceptron (MLP)\n",
        "    '''\n",
        "\n",
        "    def __init__(self, size_layers, act_funct='sigmoid', reg_lambda=0, bias_flag=True):\n",
        "        '''\n",
        "        Constructor method. Defines the characteristics of the MLP\n",
        "        Arguments:\n",
        "            size_layers : List with the number of Units for:\n",
        "                [Input, Hidden1, Hidden2, ... HiddenN, Output] Layers.\n",
        "            act_funtc   : Activation function for all the Units in the MLP\n",
        "                default = 'sigmoid'\n",
        "            reg_lambda: Value of the regularization parameter Lambda\n",
        "                default = 0, i.e. no regularization\n",
        "            bias: Indicates is the bias element is added for each layer, but the output\n",
        "        '''\n",
        "        self.size_layers = size_layers\n",
        "        self.n_layers    = len(size_layers)\n",
        "        self.act_f       = act_funct\n",
        "        self.lambda_r    = reg_lambda\n",
        "        self.bias_flag   = bias_flag\n",
        "\n",
        "        # Ramdomly initialize theta (MLP weights)\n",
        "        self.initialize_theta_weights()\n",
        "\n",
        "    def initialize_theta_weights(self):\n",
        "        '''\n",
        "        Initialize theta_weights, initialization method depends\n",
        "        on the Activation Function and the Number of Units in the current layer\n",
        "        and the next layer.\n",
        "        The weights for each layer as of the size [next_layer, current_layer + 1]\n",
        "        '''\n",
        "        self.theta_weights = []\n",
        "        size_next_layers = self.size_layers.copy()\n",
        "        size_next_layers.pop(0)\n",
        "        for size_layer, size_next_layer in zip(self.size_layers, size_next_layers):\n",
        "            if self.act_f == 'sigmoid':\n",
        "                # Method presented \"Understanding the difficulty of training deep feedforward neurla networks\"\n",
        "                # Xavier Glorot and Youshua Bengio, 2010\n",
        "                epsilon = 4.0 * np.sqrt(6) / np.sqrt(size_layer + size_next_layer)\n",
        "                # Weigts from a uniform distribution [-epsilon, epsion]\n",
        "                if self.bias_flag:\n",
        "                    theta_tmp = epsilon * ( (np.random.rand(size_next_layer, size_layer + 1) * 2.0 ) - 1)\n",
        "                else:\n",
        "                    theta_tmp = epsilon * ( (np.random.rand(size_next_layer, size_layer) * 2.0 ) - 1)\n",
        "            elif self.act_f == 'relu':\n",
        "                # Method presented in \"Delving Deep into Rectifiers: Surpassing Human-Level Performance on ImageNet Classfication\"\n",
        "                # He et Al. 2015\n",
        "                epsilon = np.sqrt(2.0 / (size_layer * size_next_layer) )\n",
        "                # Weigts from Normal distribution mean = 0, std = epsion\n",
        "                if self.bias_flag:\n",
        "                    theta_tmp = epsilon * (np.random.randn(size_next_layer, size_layer + 1 ))\n",
        "                else:\n",
        "                    theta_tmp = epsilon * (np.random.randn(size_next_layer, size_layer))\n",
        "            self.theta_weights.append(theta_tmp)\n",
        "        return self.theta_weights\n",
        "\n",
        "    def train(self, X, Y, iterations=400, reset=False):\n",
        "        '''\n",
        "        Given X (feature matrix) and y (class vector)\n",
        "        Updates the Theta Weights by running Backpropagation N tines\n",
        "        Arguments:\n",
        "            X          : Feature matrix [n_examples, n_features]\n",
        "            Y          : Sparse class matrix [n_examples, classes]\n",
        "            iterations : Number of times Backpropagation is performed\n",
        "                default = 400\n",
        "            reset      : If set, initialize Theta Weights before training\n",
        "                default = False\n",
        "        '''\n",
        "\n",
        "        if reset:\n",
        "            self.initialize_theta_weights()\n",
        "        for iteration in range(iterations):\n",
        "            self.gradients = self.backpropagation(X, Y)\n",
        "            self.gradients_vector = self.unroll_weights(self.gradients)\n",
        "            self.theta_vector = self.unroll_weights(self.theta_weights)\n",
        "            self.theta_vector = self.theta_vector - self.gradients_vector\n",
        "            self.theta_weights = self.roll_weights(self.theta_vector)\n",
        "\n",
        "    def predict(self, X):\n",
        "        '''\n",
        "        Given X (feature matrix), y_hay is computed\n",
        "        Arguments:\n",
        "            X      : Feature matrix [n_examples, n_features]\n",
        "        Output:\n",
        "            y_hat  : Computed Vector Class for X\n",
        "        '''\n",
        "        A , Z = self.feedforward(X)\n",
        "        Y_hat = A[-1]\n",
        "        return Y_hat\n",
        "\n",
        "    def backpropagation(self, X, Y):\n",
        "        '''\n",
        "        Implementation of the Backpropagation algorithm with regularization\n",
        "        '''\n",
        "        if self.act_f == 'sigmoid':\n",
        "            g_dz = lambda x: self.sigmoid_derivative(x)\n",
        "        elif self.act_f == 'relu':\n",
        "            g_dz = lambda x: self.relu_derivative(x)\n",
        "\n",
        "        n_examples = X.shape[0]\n",
        "        # Feedforward\n",
        "        A, Z = self.feedforward(X)\n",
        "\n",
        "        # Backpropagation\n",
        "        deltas = [None] * self.n_layers\n",
        "        deltas[-1] = A[-1] - Y\n",
        "        # For the second last layer to the second one\n",
        "        for ix_layer in np.arange(self.n_layers - 1 - 1 , 0 , -1):\n",
        "            theta_tmp = self.theta_weights[ix_layer]\n",
        "            if self.bias_flag:\n",
        "                # Removing weights for bias\n",
        "                theta_tmp = np.delete(theta_tmp, np.s_[0], 1)\n",
        "            deltas[ix_layer] = (np.matmul(theta_tmp.transpose(), deltas[ix_layer + 1].transpose() ) ).transpose() * g_dz(Z[ix_layer])\n",
        "\n",
        "        # Compute gradients\n",
        "        gradients = [None] * (self.n_layers - 1)\n",
        "        for ix_layer in range(self.n_layers - 1):\n",
        "            grads_tmp = np.matmul(deltas[ix_layer + 1].transpose() , A[ix_layer])\n",
        "            grads_tmp = grads_tmp / n_examples\n",
        "            if self.bias_flag:\n",
        "                # Regularize weights, except for bias weigths\n",
        "                grads_tmp[:, 1:] = grads_tmp[:, 1:] + (self.lambda_r / n_examples) * self.theta_weights[ix_layer][:,1:]\n",
        "            else:\n",
        "                # Regularize ALL weights\n",
        "                grads_tmp = grads_tmp + (self.lambda_r / n_examples) * self.theta_weights[ix_layer]\n",
        "            gradients[ix_layer] = grads_tmp;\n",
        "        return gradients\n",
        "\n",
        "    def feedforward(self, X):\n",
        "        '''\n",
        "        Implementation of the Feedforward\n",
        "        '''\n",
        "        if self.act_f == 'sigmoid':\n",
        "            g = lambda x: self.sigmoid(x)\n",
        "        elif self.act_f == 'relu':\n",
        "            g = lambda x: self.relu(x)\n",
        "\n",
        "        A = [None] * self.n_layers\n",
        "        Z = [None] * self.n_layers\n",
        "        input_layer = X\n",
        "\n",
        "        for ix_layer in range(self.n_layers - 1):\n",
        "            n_examples = input_layer.shape[0]\n",
        "            if self.bias_flag:\n",
        "                # Add bias element to every example in input_layer\n",
        "                input_layer = np.concatenate((np.ones([n_examples ,1]) ,input_layer), axis=1)\n",
        "            A[ix_layer] = input_layer\n",
        "            # Multiplying input_layer by theta_weights for this layer\n",
        "            Z[ix_layer + 1] = np.matmul(input_layer,  self.theta_weights[ix_layer].transpose() )\n",
        "            # Activation Function\n",
        "            output_layer = g(Z[ix_layer + 1])\n",
        "            # Current output_layer will be next input_layer\n",
        "            input_layer = output_layer\n",
        "\n",
        "        A[self.n_layers - 1] = output_layer\n",
        "        return A, Z\n",
        "\n",
        "\n",
        "    def unroll_weights(self, rolled_data):\n",
        "        '''\n",
        "        Unroll a list of matrices to a single vector\n",
        "        Each matrix represents the Weights (or Gradients) from one layer to the next\n",
        "        '''\n",
        "        unrolled_array = np.array([])\n",
        "        for one_layer in rolled_data:\n",
        "            unrolled_array = np.concatenate((unrolled_array, one_layer.flatten('F')) )\n",
        "        return unrolled_array\n",
        "\n",
        "    def roll_weights(self, unrolled_data):\n",
        "        '''\n",
        "        Unrolls a single vector to a list of matrices\n",
        "        Each matrix represents the Weights (or Gradients) from one layer to the next\n",
        "        '''\n",
        "        size_next_layers = self.size_layers.copy()\n",
        "        size_next_layers.pop(0)\n",
        "        rolled_list = []\n",
        "        if self.bias_flag:\n",
        "            extra_item = 1\n",
        "        else:\n",
        "            extra_item = 0\n",
        "        for size_layer, size_next_layer in zip(self.size_layers, size_next_layers):\n",
        "            n_weights = (size_next_layer * (size_layer + extra_item))\n",
        "            data_tmp = unrolled_data[0 : n_weights]\n",
        "            data_tmp = data_tmp.reshape(size_next_layer, (size_layer + extra_item), order = 'F')\n",
        "            rolled_list.append(data_tmp)\n",
        "            unrolled_data = np.delete(unrolled_data, np.s_[0:n_weights])\n",
        "        return rolled_list\n",
        "\n",
        "    def sigmoid(self, z):\n",
        "        '''\n",
        "        Sigmoid function\n",
        "        z can be an numpy array or scalar\n",
        "        '''\n",
        "        result = 1.0 / (1.0 + np.exp(-z))\n",
        "        return result\n",
        "\n",
        "    def relu(self, z):\n",
        "        '''\n",
        "        Rectified Linear function\n",
        "        z can be an numpy array or scalar\n",
        "        '''\n",
        "        if np.isscalar(z):\n",
        "            result = np.max((z, 0))\n",
        "        else:\n",
        "            zero_aux = np.zeros(z.shape)\n",
        "            meta_z = np.stack((z , zero_aux), axis = -1)\n",
        "            result = np.max(meta_z, axis = -1)\n",
        "        return result\n",
        "\n",
        "    def sigmoid_derivative(self, z):\n",
        "        '''\n",
        "        Derivative for Sigmoid function\n",
        "        z can be an numpy array or scalar\n",
        "        '''\n",
        "        result = self.sigmoid(z) * (1 - self.sigmoid(z))\n",
        "        return result\n",
        "\n",
        "    def relu_derivative(self, z):\n",
        "        '''\n",
        "        Derivative for Rectified Linear function\n",
        "        z can be an numpy array or scalar\n",
        "        '''\n",
        "        result = 1 * (z > 0)\n",
        "        return result\n",
        "\n",
        "\n",
        ""
      ],
      "metadata": {
        "id": "a55wPln4JiQG"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#  Practical example"
      ],
      "metadata": {
        "id": "iE7EJrR2WVIQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "This time, an MLP network will be used, using the code developed in the previous practical\n",
        "example, including this one as a library."
      ],
      "metadata": {
        "id": "Xj1OmtcKWTv-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pickle,gzip\n",
        "import matplotlib.pyplot as plt\n",
        "import os\n",
        "import urllib.request"
      ],
      "metadata": {
        "id": "D4y7r-zHWcoa"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "mnist_filename = 'mnist.pkl'\n",
        "\n",
        "# As ‘mnist.pkl.gz' was created in Python2, ‘latin1' encoding is needed to loaded in Python3\n",
        "with open(mnist_filename, 'rb') as f:\n",
        "  train_set, valid_set, test_set = pickle.load(f, encoding='latin1')"
      ],
      "metadata": {
        "id": "uQZ0FaKDdw0W"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Show 8 random data from the database with which we will work."
      ],
      "metadata": {
        "id": "Ga1YaUMHdyAh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Plot random examples\n",
        "examples = np.random. randint(10000, size=8)\n",
        "n_examples = len(examples)\n",
        "plt.figure()\n",
        "\n",
        "for ix_example in range(n_examples):\n",
        "  tmp = np.reshape(train_set[0] [examples [ix_example],:], [28,28])\n",
        "  ax = plt.subplot(1,n_examples, ix_example + 1)\n",
        "  ax. set_yticklabels([])\n",
        "  ax. set_xticklabels([])\n",
        "  plt.title(str(train_set[1] [examples [ix_example] ]))\n",
        "  plt.imshow(tmp, cmap='gray')"
      ],
      "metadata": {
        "id": "EGzBePGhecnf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Split the data into data used for training and data used for testing."
      ],
      "metadata": {
        "id": "xrBAJEGDeewA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Training data\n",
        "train_X = valid_set[0]\n",
        "train_y = valid_set[1]\n",
        "print('Shape of training set: ' + str(train_X. shape) )\n",
        "\n",
        "\n",
        "# change y [1D] to Y [2D] sparse array coding class\n",
        "n_examples = len(train_y)\n",
        "labels = np.unique(train_y)\n",
        "train_Y = np.zeros((n_examples, len(labels)))\n",
        "for ix_label in range(len(labels)):\n",
        "  # Find examples with with a Label = lables(ix_label)\n",
        "  ix_tmp = np.where(train_y == labels [ix_label]) [0]\n",
        "  train_Y[ix_tmp, ix_label] = 1\n",
        "\n",
        "# Test data\n",
        "test_X = test_set[0]\n",
        "test_y = test_set[1]\n",
        "\n",
        "print ('Shape of test set: ' + str(test_X.shape))\n",
        "\n",
        "# change y [1D] to Y [2D] sparse array coding class\n",
        "n_examples = len(test_y)\n",
        "labels = np_unique(test_y)\n",
        "\n",
        "test_Y = np.zeros((n_examples, len(labels)))\n",
        "\n",
        "for ix_label in range(len(labels)):\n",
        "  # Find examples with with a Label = lables(ix_label)\n",
        "  ix_tmp = np.where(test_y == labels[ix_label]) [0]\n",
        "  test_Y[ix_tmp, ix_label] = 1"
      ],
      "metadata": {
        "id": "YvpVb8RCgBgX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Create the multilayer perceptron network with 4 layers (input, two hidden and output) and use the\n",
        "“relu” function as the activation function in all of them."
      ],
      "metadata": {
        "id": "IZT06D97gCVX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Creating the MLP object initialize the\n",
        "mlp_classifier = Mlp(size_layers = [784, 25, 10, 10],act_funct = 'relu',reg_lambda = 6,bias_flag = True)"
      ],
      "metadata": {
        "id": "_hvrTgV-mduG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Train the network with the training data."
      ],
      "metadata": {
        "id": "DiJFEiQHmmNz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Training with Backpropagation and 460 iterations\n",
        "iterations = 400\n",
        "loss = np.zeros([iterations,1])\n",
        "for ix in range(iterations):\n",
        "  mlp_classifier.train(train_X, train_Y, 1)\n",
        "  Y_hat = mlp_classifier.predict(train_X)\n",
        "  y_tmp = np.argmax(Y_hat, axis=1)\n",
        "  _hat = labels[y_tmp]\n",
        "  loss[ix] = (0.5)*np.square(y_hat - train_y).mean()\n"
      ],
      "metadata": {
        "id": "pvqu33UBmm_W"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Plot the evolution of the cost function."
      ],
      "metadata": {
        "id": "GdqUq9bKmuDU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Ploting loss vs iterations\n",
        "plt.figure()\n",
        "ix = np.arange(iterations)\n",
        "plt.plot(ix, loss)"
      ],
      "metadata": {
        "id": "ZGYjXtDXmzlq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Show some of the results obtained."
      ],
      "metadata": {
        "id": "fOSU-_JVm2Wk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Some test samples, [T]rue labels and [P]redicted labels\n",
        "examples = np.random. randint(10000, size=8)\n",
        "n_examples = len(examples)\n",
        "plt.figure()\n",
        "\n",
        "for ix_example in range(n_examples):\n",
        "  tmp = np.reshape(test_X[examples[ix_example],:], [28,28])\n",
        "  ax = plt.subplot(1,8, ix_example + 1)\n",
        "  ax. set_yticklabels([])\n",
        "  ax. set_xticklabels([])\n",
        "  plt.title('T'+ str(test_y[examples[ix_example]]) + ', P' + str(y_hat[examples[ix_example]]))\n",
        "  plt.imshow(tmp, cmap='gray')"
      ],
      "metadata": {
        "id": "-PAr9Rfnm3hl"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}