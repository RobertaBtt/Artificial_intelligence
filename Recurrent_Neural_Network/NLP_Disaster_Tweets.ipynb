{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Natural Language Processing - Identify disaster Tweets"
      ],
      "metadata": {
        "id": "iMnKm58KxKD_"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "91kXi4R9vWpu"
      },
      "outputs": [],
      "source": [
        "import re\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "import gensim.downloader as api\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.layers import Dense, LSTM, Dropout\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import f1_score, confusion_matrix, precision_score, recall_score,log_loss\n",
        "from imblearn.under_sampling import RandomUnderSampler\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "# =============================================================================\n",
        "# 0. General functions\n",
        "# =============================================================================\n",
        "def word_vector(df_input, lemmatizer, word_vectors, vocabulary, col_sentences):\n",
        " \"\"\"\n",
        " Function to preprocess the input words and get a list with\n",
        " The embeddings arrays of the words in each record.\n",
        " Parameters\n",
        " ----------\n",
        " df_input : dataframe\n",
        " input dataframe with all texts.\n",
        " lemmatizer : object\n",
        " NLTK stemming object.\n",
        " word_vectors : object\n",
        " Object with the word2vecs of the Gensim vocabulary.\n",
        " vocabulary : list\n",
        " list of existing words in Gensim's vocabulary.\n",
        " col_sentences : str\n",
        " column of the dataframe where the phrases are.\n",
        " Returns\n",
        " -------\n",
        " X : list\n",
        " List of lists in which each record has the list with the arrays\n",
        " of the embeddings of the words of that phrase. That is, X[0] has\n",
        "\n",
        " a list where each element corresponds to the embeddings of a word.\n",
        " Thus, for example, X[0][2] will be a vector of dimension 100 where it appears\n",
        " the vector of embeddings of the third word of the first sentence.\n",
        " \"\"\"\n",
        "\n",
        "\n",
        " X = []\n",
        "\n",
        " for text in df_input[col_sentences]:\n",
        "\n",
        "    # Tokenize every phrase\n",
        "    words = re.findall(r'\\w+', text.lower(),flags = re.UNICODE)\n",
        "    # Elimination of stop_words\n",
        "    words = [word for word in words if word not in stopwords.words('english')]\n",
        "    # Remove hyphens and other weird symbols\n",
        "    words = [word for word in words if not word.isdigit()] # Elimino numeros\n",
        "    # Stemming\n",
        "    words = [lemmatizer.lemmatize(w) for w in words]\n",
        "    # Delete words that are not in the vocabulary\n",
        "    words = [word for word in words if word in vocabulary]\n",
        "    # Word2Vec\n",
        "    words_embeddings = [word_vectors[x] for x in words]\n",
        "\n",
        "    # Save the final sentence\n",
        "    X.append(words_embeddings) # save as a numpy array\n",
        " return X"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def create_RNN(x_train, K, n_lstm=8, loss='categorical_crossentropy', optimizer='adam'):\n",
        " \"\"\"\n",
        " Function to create the RNN. As input parameter we only need the array\n",
        " of features to specify the input dimensionality of the NN.\n",
        " Parameters\n",
        " ----------\n",
        " x_input : array\n",
        " Matrix of input features.\n",
        " K: int\n",
        " Exit classes\n",
        " n_lstm : int, optional\n",
        " Number of lstm used. The default is 8.\n",
        " loss : string, optional\n",
        " loss metric. The default is 'categorical_crossentropy'.\n",
        " optimizer : string, optional\n",
        " optimizer. The default is 'adam'.\n",
        " Returns\n",
        " -------\n",
        " model : object\n",
        " Trained model.\n",
        "\"\"\"\n",
        "\n",
        "  # Begin sequence\n",
        " model = tf.keras.Sequential()\n",
        "\n",
        " # Add a LSTM layer with 8 internal units.\n",
        " model.add(LSTM(n_lstm, input_shape=x_train.shape[-2:]))\n",
        "\n",
        " # Add Dropout\n",
        " # model.add(Dropout(0.5))\n",
        "\n",
        " # # Another layer\n",
        " # model.add(Dense(100, activation='relu'))\n",
        "\n",
        " # # Output\n",
        " model.add(Dense(K, activation='sigmoid'))\n",
        "\n",
        " # Compile model\n",
        " model.compile(loss=loss, optimizer=optimizer)\n",
        "\n",
        " return model"
      ],
      "metadata": {
        "id": "p7CRH11Mxn3_"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "As a next step, we load the exercise data. As the exercice mentions, the dataset consists of a\n",
        "table with tweets that may or may not be talking about natural disasters. The idea is therefore to\n",
        "build a tweet classifier that allows detecting when users are talking about it (and distinguishing\n",
        "those tweets from others) in order to act on it.These types of applications, identified within what\n",
        "is called Big Data For Social Good, contribute positively to society, and allow complementing the information available with certain systems (e.g. seismographs, weather predictors ...) with\n",
        "insights derived from unstructured data in real time, as is the case with tweets."
      ],
      "metadata": {
        "id": "SV4CTKJrxyeo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#==========================================================================\n",
        "# 1. Load Data\n",
        "#==========================================================================\n",
        "# Load files\n",
        "tf.random.set_seed(42)\n",
        "path_files = \"predict-disaster\"\n",
        "df_raw = pd.read_csv('train.csv', encoding = \"latin-1\")\n",
        "df_raw = df_raw[['text', 'target']]\n",
        "print(df_raw)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7WdfFTk_yAUm",
        "outputId": "6fc8371e-0825-4055-8ab3-e4f00e21fedc"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                                                   text  target\n",
            "0     Our Deeds are the Reason of this #earthquake M...       1\n",
            "1                Forest fire near La Ronge Sask. Canada       1\n",
            "2     All residents asked to 'shelter in place' are ...       1\n",
            "3     13,000 people receive #wildfires evacuation or...       1\n",
            "4     Just got sent this photo from Ruby #Alaska as ...       1\n",
            "...                                                 ...     ...\n",
            "7608  Two giant cranes holding a bridge collapse int...       1\n",
            "7609  @aria_ahrary @TheTawniest The out of control w...       1\n",
            "7610  M1.94 [01:04 UTC]?5km S of Volcano Hawaii. htt...       1\n",
            "7611  Police investigating after an e-bike collided ...       1\n",
            "7612  The Latest: More Homes Razed by Northern Calif...       1\n",
            "\n",
            "[7613 rows x 2 columns]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Thus, tweets appear within these two categories. An example of a tweet that talks about natural\n",
        "disasters is the following:"
      ],
      "metadata": {
        "id": "5f2Op5vTzePn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(df_raw['text'][10])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fclgfN6CzeIt",
        "outputId": "1fa3f487-b419-4aee-b5fe-f6ec250002d3"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Three people died from the heat wave so far\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "Instead, a tweet that doesn't talk about it is:"
      ],
      "metadata": {
        "id": "J2QpXq0Hz2zE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(df_raw['text'][16])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JuaSERnRzd8i",
        "outputId": "8bc89650-7c16-447a-8fd3-6021597c1430"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "I love fruits\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(df_raw['target'].value_counts() / len(df_raw))\n",
        ""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JQjZgGYG0Qaq",
        "outputId": "e0018286-ca1f-4e38-f9c6-d86f26eec87b"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0    0.57034\n",
            "1    0.42966\n",
            "Name: target, dtype: float64\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Shuffle input\n",
        "df_raw = df_raw.sample(frac=1)\n",
        "# Load word2vec\n",
        "word_vectors = api.load(\"glove-wiki-gigaword-100\")\n",
        "vocabulary = [x for x in word_vectors.vocab]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 385
        },
        "id": "qUBtsO250kvo",
        "outputId": "ee320510-90dc-4fe6-c6cf-114f4593cbc2"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "error",
          "ename": "AttributeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-17-c561f3c70b46>\u001b[0m in \u001b[0;36m<cell line: 5>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;31m# Load word2vec\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mword_vectors\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mapi\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"glove-wiki-gigaword-100\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mvocabulary\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mx\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mword_vectors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvocab\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/gensim/models/keyedvectors.py\u001b[0m in \u001b[0;36mvocab\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    732\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    733\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mvocab\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 734\u001b[0;31m         raise AttributeError(\n\u001b[0m\u001b[1;32m    735\u001b[0m             \u001b[0;34m\"The vocab attribute was removed from KeyedVector in Gensim 4.0.0.\\n\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    736\u001b[0m             \u001b[0;34m\"Use KeyedVector's .key_to_index dict, .index_to_key list, and methods \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAttributeError\u001b[0m: The vocab attribute was removed from KeyedVector in Gensim 4.0.0.\nUse KeyedVector's .key_to_index dict, .index_to_key list, and methods .get_vecattr(key, attr) and .set_vecattr(key, attr, new_val) instead.\nSee https://github.com/RaRe-Technologies/gensim/wiki/Migrating-from-Gensim-3.x-to-4"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "In this case, the output class (binary value) is already expressed numerically, so it is not\n",
        "necessary to do LabelEncoding. Also, being a binary variable, it is not necessary to do\n",
        "OneHotEncoding either."
      ],
      "metadata": {
        "id": "ymrW5T5O04cg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "6op74GJv04JN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Set lemmatizer\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "# X/y split\n",
        "X = pd.DataFrame(df_raw['text'])\n",
        "y = df_raw['target']"
      ],
      "metadata": {
        "id": "tpEkRhq11DdK"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Subsequently, we pre-process the tweets to express them in numerical format by embeddings\n",
        "of the individual words, after having eliminated stopwords, having lemmatized, eliminated words\n",
        "that are not in the vocabulary and eliminated numerical characters."
      ],
      "metadata": {
        "id": "E6zzCFG_1HTy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#==========================================================================\n",
        "# 2. Preprocess\n",
        "#=========================================================================\n",
        "\n",
        "# Obtain X variable and prepare y.\n",
        "X = word_vector(X,\n",
        " lemmatizer,\n",
        " word_vectors,\n",
        " vocabulary,\n",
        " col_sentences=\"text\")"
      ],
      "metadata": {
        "id": "5PGCBmmv1HFe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "After that, we make the separation between the train dataset and the one that we will use for the\n",
        "test, and we define the maximum size of the sequence per tweet. We take the maximum\n",
        "sequence size as a reference to encompass the size of the entire tweet for 99% of the tweets in\n",
        "the dataset. We apply a subsequent padding to fill the shorter tweets with null values up to that\n",
        "maximum length size, and we truncate the largest tweets until we have that maximum size,\n",
        "eliminating at the end of the tweet."
      ],
      "metadata": {
        "id": "lVJv4i4n1WVx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Train/Test split\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        " X, y, test_size=0.25, random_state=42)\n",
        "# Obtain tensor: [N_SENTENCES x SEQ_LENGTH x EMBEDDING_FEATURES]\n",
        "SEQ_LENGTH = np.int(np.round(np.percentile([len(x) for x in X], 99, interpolation =\n",
        "'midpoint')))\n",
        "data_train = pad_sequences(X_train,\n",
        " maxlen=SEQ_LENGTH,\n",
        " padding=\"post\",\n",
        " truncating=\"post\")\n",
        "data_test = pad_sequences(X_test,\n",
        " maxlen=SEQ_LENGTH,\n",
        " padding=\"post\",\n",
        " truncating=\"post\")"
      ],
      "metadata": {
        "id": "8G8bsptz03zT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "In a first iteration, we get the output for the following configuration: batch_size = 200, epochs =\n",
        "50, optimize = adam, n_lstm = 50. In this case, since it is a binary problem, K = 1."
      ],
      "metadata": {
        "id": "E5A58oxw1cxp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#========================================================================\n",
        "# 3. Train model\n",
        "# ========================================================================\n",
        "# Params\n",
        "K = 1\n",
        "batch_size = 200\n",
        "epochs = 50\n",
        "# Create RNN\n",
        "model = create_RNN(x_train = data_train, K = K, n_lstm = 50, loss = 'binary_crossentropy', optimizer = 'adam')\n",
        "print(model.summary())\n",
        "# Fit model\n",
        "model.fit(data_train, y_train, epochs = epochs, batch_size = batch_size)\n",
        "# Save model\n",
        "model.save('model_nlp_disaster.h5')"
      ],
      "metadata": {
        "id": "od20Ozit1chz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "After training and storing the model, we obtain the predictions along with different metrics:\n",
        "confusion matrix, precision, recall and F1. The predictions, having used the sigmoid function,\n",
        "will be expressed as a continuous value between 0 and 1. We round this value (with a standard\n",
        "threshold of 0.5) to see the final class associated with each prediction."
      ],
      "metadata": {
        "id": "3WIYiWUz1qVM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ========================================================================\n",
        "# 4. Evaluate\n",
        "# ========================================================================\n",
        "# Obtain predictions\n",
        "y_pred = model.predict(data_test)\n",
        "# Round predictions\n",
        "y_pred = y_pred.round()\n",
        "y_pred = [x[0] for x in y_pred]\n",
        "y_test = list(y_test.values)\n",
        "# Evaluate results\n",
        "cm = confusion_matrix(y_test, y_pred)\n",
        "\n",
        "print(\"Confusion Matrix: \", cm)\n",
        "print(\"Precision: \", np.round(precision_score(y_test, y_pred, average='macro'), 4))\n",
        "print(\"Recall: \", np.round(recall_score(y_test, y_pred, average='macro'), 4))\n",
        "print(\"f1_score: \", np.round(f1_score(y_test, y_pred, average='macro'), 4))"
      ],
      "metadata": {
        "id": "bMF-XSOe1tnM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can see how the metrics are approximately the same with all the configurations that use\n",
        "ADAM as an optimization function, and we only notice that they get substantially worse when\n",
        "changing the algorithm to SGD. In this way, we would opt for the last configuration, since the\n",
        "metrics are similar and instead its configuration is much simpler, greatly reducing the\n",
        "computational cost."
      ],
      "metadata": {
        "id": "O-xa0_-A125q"
      }
    }
  ]
}